{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd27944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "sys.path.insert(0, '..')\n",
    "from models import PointNet, DGCNNSegBackbone\n",
    "from datasets import PointCloudNormalize, ABCDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.training_routines import RunningMetrics\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837578c",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607b5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "exp_id = 'z52pya7i'\n",
    "n_epochs = 50\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49891fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalsDataset(Dataset):\n",
    "    def __init__(self, file, transform=None, sample_frac=1, seed=42):\n",
    "        super().__init__()\n",
    "                    \n",
    "        with h5py.File(file, 'r') as f:\n",
    "            self.points = f['points'][:]\n",
    "            self.normals = f['point_normals'][:]\n",
    "        \n",
    "        normals_lens = (self.normals**2).sum(axis=2, keepdims=True)**0.5\n",
    "        self.normals /= (normals_lens + 1e-8)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if sample_frac < 1:\n",
    "            n_samples = int(self.normals.shape[0] * sample_frac)\n",
    "            r = np.random.RandomState(seed)\n",
    "            idx = r.permutation(self.normals.shape[0])[:n_samples]\n",
    "            self.points = self.points[idx]\n",
    "            self.normals = self.normals[idx]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pc = self.points[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            pc = self.transform(pc)\n",
    "            \n",
    "        return pc.T, self.normals[idx].T\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "    \n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, backbone, finetune_head=False):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if finetune_head:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(self.backbone.n_output_point, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(256, 3, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone.forward_features(x)\n",
    "        return self.head(features)\n",
    "    \n",
    "def compute_loss(gt, pred):\n",
    "    pred = F.normalize(pred, dim=1)\n",
    "    cos_sim = torch.einsum('nck, nck -> nk', gt, pred)\n",
    "    \n",
    "    return (1 - cos_sim.pow_(2)).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    bar = tqdm(loader, desc='val')\n",
    "    metrics = RunningMetrics()\n",
    "    \n",
    "    for x, gt in bar:\n",
    "        pred = model(x.to(device))\n",
    "        preds.append(pred.cpu())\n",
    "        gts.append(gt)\n",
    "        metrics.step({'loss': compute_loss(gt, pred.cpu())})\n",
    "        bar.set_postfix(metrics.report())\n",
    "    \n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    gts = torch.cat(gts, dim=0)\n",
    "    loss = compute_loss(gts, preds).item()\n",
    "    print(metrics.report())\n",
    "    \n",
    "    print('val loss', loss)\n",
    "    return loss\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, scheduler, n_epochs, val_every=1):\n",
    "    val_loss_list = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        bar = tqdm(train_loader)\n",
    "        model.train()\n",
    "        metrics = RunningMetrics()\n",
    "        \n",
    "        for x, gt in bar:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x.to(device))\n",
    "            loss = compute_loss(gt.to(device), pred) + 0.001 * model.backbone.reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metrics.step({'loss': loss})\n",
    "            report = metrics.report()\n",
    "            report.update({'epoch': epoch})\n",
    "            bar.set_postfix(report)\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % val_every == 0:\n",
    "            val_loss = validate(model, test_loader)\n",
    "    \n",
    "    val_loss_list.append(val_loss)\n",
    "    return val_loss_list\n",
    "            \n",
    "def get_model(exp_id, n_epochs, finetune_head, lr, weight_decay):\n",
    "    model = PointNet()\n",
    "    # model = DGCNNSegBackbone()\n",
    "    if exp_id is not None:\n",
    "        checkpoint_path = f'../weights/simclr_run_{exp_id}_ckp_150.pt'\n",
    "        state = torch.load(checkpoint_path, map_location='cpu')['model']\n",
    "        model.load_state_dict(state)\n",
    "        \n",
    "    model = Regressor(model, finetune_head).to(device)\n",
    "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad,\n",
    "                                        model.parameters()),\n",
    "                                 lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs * len(train_loader), eta_min=0)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3e96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../datasets/hdfs/train_0.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5b470",
   "metadata": {},
   "source": [
    "## Simple training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb472da4",
   "metadata": {},
   "source": [
    "### Only head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567064f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ABCDataset(dataset_path,\n",
    "                      'train',\n",
    "                      'normals',\n",
    "                      transform=PointCloudNormalize('box'))\n",
    "test_ds = ABCDataset(dataset_path,\n",
    "                     'test',\n",
    "                     'normals',\n",
    "                     transform=PointCloudNormalize('box'))\n",
    "\n",
    "train_loader = DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "test_loader = DataLoader(test_ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cb466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:39<00:00,  4.54it/s, loss=0.0322, epoch=1]\n",
      "val: 100%|██████████| 154/154 [00:12<00:00, 12.70it/s, loss=0.0116]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.011616361338834794}\n",
      "val loss 0.028366807180088046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.66it/s, loss=0.00982, epoch=2]\n",
      "val: 100%|██████████| 154/154 [00:13<00:00, 11.22it/s, loss=0.00977]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.009765839555065802}\n",
      "val loss 0.024207011688957673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.66it/s, loss=0.00849, epoch=3]\n",
      "val: 100%|██████████| 154/154 [00:11<00:00, 12.84it/s, loss=0.00884]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.008842138788896794}\n",
      "val loss 0.021340657224789387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.67it/s, loss=0.00813, epoch=4]\n",
      "val: 100%|██████████| 154/154 [00:12<00:00, 12.42it/s, loss=0.00941]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.00940612205951698}\n",
      "val loss 0.021402344356352607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.65it/s, loss=0.00732, epoch=5]\n",
      "val: 100%|██████████| 154/154 [00:11<00:00, 12.97it/s, loss=0.00719]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.00718526574809981}\n",
      "val loss 0.017258499373506035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.62it/s, loss=0.00662, epoch=6]\n",
      "val: 100%|██████████| 154/154 [00:12<00:00, 12.59it/s, loss=0.00698]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006980279989205926}\n",
      "val loss 0.016801920932681742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.64it/s, loss=0.0061, epoch=7] \n",
      "val: 100%|██████████| 154/154 [00:12<00:00, 11.85it/s, loss=0.00969]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.009693494002728172}\n",
      "val loss 0.02292255282601441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:39<00:00,  4.61it/s, loss=0.00621, epoch=8]\n",
      "val: 100%|██████████| 154/154 [00:11<00:00, 13.02it/s, loss=0.00889]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.00889082900909806}\n",
      "val loss 0.024843736338284166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.64it/s, loss=0.00577, epoch=9]\n",
      "val: 100%|██████████| 154/154 [00:12<00:00, 12.56it/s, loss=0.0068] \n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006798417233047038}\n",
      "val loss 0.016563965764451852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.64it/s, loss=0.00559, epoch=10]\n",
      "val: 100%|██████████| 154/154 [00:11<00:00, 12.98it/s, loss=0.00629]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006287164666074511}\n",
      "val loss 0.015272702275965036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.64it/s, loss=0.00535, epoch=11]\n",
      "val: 100%|██████████| 154/154 [00:12<00:00, 12.59it/s, loss=0.00613]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006127076070280225}\n",
      "val loss 0.01464462086908843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.62it/s, loss=0.00504, epoch=12]\n",
      "val: 100%|██████████| 154/154 [00:11<00:00, 12.94it/s, loss=0.00673]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006731700745858091}\n",
      "val loss 0.016643311328017194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:38<00:00,  4.63it/s, loss=0.00506, epoch=13]\n",
      "val: 100%|██████████| 154/154 [00:11<00:00, 12.97it/s, loss=0.00652]\n",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0065170104715226804}\n",
      "val loss 0.016235378344882583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/180 [00:01<00:37,  4.54it/s, loss=0.00424, epoch=14]"
     ]
    }
   ],
   "source": [
    "finetune_head = True\n",
    "exp_id = 'z52pya7i'\n",
    "model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "val_loss1 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = True\n",
    "exp_id = None\n",
    "model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "val_loss2 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527685a",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a731bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = False\n",
    "exp_id = 'z52pya7i'\n",
    "model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "val_loss3 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ae572",
   "metadata": {},
   "source": [
    "### From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b687ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = False\n",
    "exp_id = None\n",
    "model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "val_loss4 = train(model, train_loader, test_loader, optimizer, scheduler, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b94269",
   "metadata": {},
   "source": [
    "## Semisupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47387290",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ABCDataset(dataset_path,\n",
    "                     'test',\n",
    "                     'normals',\n",
    "                     transform=PointCloudNormalize('box'))\n",
    "\n",
    "test_loader = DataLoader(test_ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8653d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = False\n",
    "exp_id = 'z52pya7i'\n",
    "n_epochs = 200\n",
    "run_results = []\n",
    "\n",
    "for i, seed in enumerate([24234, 23214, 64645]):\n",
    "    train_ds = ABCDataset(dataset_path,\n",
    "                          'train',\n",
    "                          'normals',\n",
    "                          transform=PointCloudNormalize('box'), sample_frac=0.01, seed=seed)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "    model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "    val_loss4 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs, val_every=50)\n",
    "    print(f'Run {i}: {val_loss4[-1]}')\n",
    "    run_results.append(val_loss4[-1])\n",
    "    \n",
    "np.mean(run_results), np.std(run_results, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = False\n",
    "exp_id = None\n",
    "n_epochs = 200\n",
    "run_results = []\n",
    "\n",
    "for i, seed in enumerate([24234, 23214, 64645]):\n",
    "    train_ds = ABCDataset(dataset_path,\n",
    "                          'train',\n",
    "                          'normals',\n",
    "                          transform=PointCloudNormalize('box'), sample_frac=0.01, seed=seed)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "\n",
    "    model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "    val_loss4 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs, val_every=50)\n",
    "    print(f'Run {i}: {val_loss4[-1]}')\n",
    "    run_results.append(val_loss4[-1])\n",
    "    \n",
    "np.mean(run_results), np.std(run_results, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = False\n",
    "exp_id = 'z52pya7i'\n",
    "n_epochs = 200\n",
    "run_results = []\n",
    "\n",
    "for i, seed in enumerate([24234, 23214, 64645]):\n",
    "    train_ds = ABCDataset(dataset_path,\n",
    "                          'train',\n",
    "                          'normals',\n",
    "                          transform=PointCloudNormalize('box'), sample_frac=0.05, seed=seed)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "\n",
    "    model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "    val_loss4 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs, val_every=50)\n",
    "    print(f'Run {i}: {val_loss4[-1]}')\n",
    "    run_results.append(val_loss4[-1])\n",
    "    \n",
    "np.mean(run_results), np.std(run_results, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea417e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_head = False\n",
    "exp_id = None\n",
    "n_epochs = 200\n",
    "run_results = []\n",
    "\n",
    "for i, seed in enumerate([24234, 23214, 64645]):\n",
    "    train_ds = ABCDataset(dataset_path,\n",
    "                          'train',\n",
    "                          'normals',\n",
    "                          transform=PointCloudNormalize('box'), sample_frac=0.05, seed=seed)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, batch_size=64)\n",
    "\n",
    "    model, optimizer, scheduler = get_model(exp_id, n_epochs, finetune_head, lr, weight_decay)\n",
    "    val_loss4 = train(model, train_loader, test_loader, optimizer, scheduler, n_epochs, val_every=50)\n",
    "    print(f'Run {i}: {val_loss4[-1]}')\n",
    "    run_results.append(val_loss4[-1])\n",
    "    \n",
    "np.mean(run_results), np.std(run_results, ddof=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
